{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file defines several useful custom layers (these are not actual keras.layers.Layer objects) to be used in the network\n",
    "You can call them like this:\n",
    "    `LayerName(...)(x)`\n",
    "\n",
    "This is useful since very often you need to chain some layers (like the classical Conv + BatchNorm + NonLinearity)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense as kDense, PReLU, ELU, LeakyReLU, Activation, Permute, Conv2DTranspose, Conv1D as kConv1D, BatchNormalization, Add, Concatenate, Multiply, Dropout, merge, Reshape, Flatten, UpSampling1D, Lambda, ZeroPadding1D, Input\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from keras.optimizers import Adam\n",
    "BATCH_NORM = 'keras'\n",
    "\n",
    "\n",
    "def Conv1D(filters, kernel_size, strides=1, padding='same', dilation_rate=1, activation=None, momentum=0.9, training=None, BN=True, config=BATCH_NORM,\n",
    "           use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "           activity_regularizer=None, kernel_constraint=None, bias_constraint=None, dropout=None, name=None, **kwargs):\n",
    "    \"\"\"conv -> BN -> activation\"\"\"\n",
    "\n",
    "    def f(x):\n",
    "        h = x\n",
    "        if dropout is not None:\n",
    "            h = Dropout(dropout)(h)\n",
    "        if padding != \"causal++\":\n",
    "            h = kConv1D(filters,\n",
    "                        kernel_size,\n",
    "                        strides=strides,\n",
    "                        padding=padding,\n",
    "                        dilation_rate=dilation_rate,\n",
    "                        activation=None,\n",
    "                        use_bias=use_bias,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        bias_regularizer=bias_regularizer,\n",
    "                        activity_regularizer=activity_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        bias_constraint=bias_constraint,\n",
    "                        **kwargs)(h)\n",
    "        else:\n",
    "            h = ZeroPadding1D(padding=(2, 0))(x)\n",
    "            h = kConv1D(filters,\n",
    "                        kernel_size,\n",
    "                        strides=strides,\n",
    "                        padding=None,\n",
    "                        activation=None,\n",
    "                        use_bias=use_bias,\n",
    "                        dilation_rate=dilation_rate,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        bias_regularizer=bias_regularizer,\n",
    "                        activity_regularizer=activity_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        bias_constraint=bias_constraint,\n",
    "                        **kwargs)(h)\n",
    "            h = Lambda(lambda x_: x_[:, :-2, :])(h)\n",
    "        h = _activation(activation, BN=BN, name=name, momentum=momentum, training=training, config=config)(h)\n",
    "        return h\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "# DENSE\n",
    "def Dense(n_units, activation=None, BN=False, channel=1, training=None, config=BATCH_NORM):\n",
    "    def f(x):\n",
    "        if len(x._keras_shape[1:]) == 2:\n",
    "            if channel == 2:\n",
    "                h = kDense(n_units)(x)\n",
    "            elif channel == 1:\n",
    "                h = Permute((2, 1))(kDense(n_units)(Permute((2, 1))(x)))\n",
    "            else:\n",
    "                raise ValueError('channel should be either 1 or 2')\n",
    "            h = _activation(activation, BN=BN, training=training, config=config)(h)\n",
    "            return h\n",
    "        elif len(x._keras_shape[1:]) == 1:\n",
    "            h = kDense(n_units)(x)\n",
    "            return _activation(activation, BN=BN, training=training, config=config)(h)\n",
    "        else:\n",
    "            raise ValueError('len(x._keras_shape) should be either 2 or 3 (including the batch dim)')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def Sum(axis):\n",
    "    def f(x):\n",
    "        return Lambda(lambda x_: K.sum(x_, axis))(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "def IsNonZero():\n",
    "    def f(x):\n",
    "        return Lambda(lambda x_: K.cast(x_ > 0, np.float32))(x)\n",
    "    return f\n",
    "\n",
    "\n",
    "# BatchNorm\n",
    "def BatchNorm(momentum=0.99, training=True):\n",
    "    def batchnorm(x, momentum=momentum, training=training):\n",
    "        return tf.layers.batch_normalization(x, momentum=momentum, training=training)\n",
    "\n",
    "    def f(x):\n",
    "        return Lambda(batchnorm, output_shape=tuple([xx for xx in x._keras_shape if xx is not None]))(x)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "# ACTIVATION\n",
    "def _activation(activation, BN=True, name=None, momentum=0.9, training=None, config=BATCH_NORM):\n",
    "    \"\"\"\n",
    "    A more general activation function, allowing to use just string (for prelu, leakyrelu and elu) and to add BN before applying the activation\n",
    "    :param training: if using a tensorflow optimizer, training should be K.learning_phase()\n",
    "                     if using a Keras optimizer, just let it to None\n",
    "    \"\"\"\n",
    "\n",
    "    def f(x):\n",
    "        if BN and activation != 'selu':\n",
    "            if config == 'keras':\n",
    "                h = BatchNormalization(momentum=momentum)(x, training=training)\n",
    "            elif config == 'tf' or config == 'tensorflow':\n",
    "                h = BatchNorm(is_training=training)(x)\n",
    "            else:\n",
    "                raise ValueError('config should be either `keras`, `tf` or `tensorflow`')\n",
    "        else:\n",
    "            h = x\n",
    "        if activation is None:\n",
    "            return h\n",
    "        if activation in ['prelu', 'leakyrelu', 'elu', 'selu']:\n",
    "            if activation == 'prelu':\n",
    "                return PReLU(name=name)(h)\n",
    "            if activation == 'leakyrelu':\n",
    "                return LeakyReLU(name=name)(h)\n",
    "            if activation == 'elu':\n",
    "                return ELU(name=name)(h)\n",
    "            if activation == 'selu':\n",
    "                return Selu()(h)\n",
    "        else:\n",
    "            h = Activation(activation, name=name)(h)\n",
    "            return h\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _selu(x):\n",
    "    \"\"\"Scaled Exponential Linear Unit. (Klambauer et al., 2017)\n",
    "    # Arguments\n",
    "        x: A tensor or variable to compute the activation function for.\n",
    "    # References\n",
    "        - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
    "    \"\"\"\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * K.elu(x, alpha)\n",
    "\n",
    "\n",
    "def Selu():\n",
    "    def f(x):\n",
    "        return Lambda(_selu)(x)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_Q_and_PI_networks(hidden_dim=10, n_actions=14):\n",
    "    hand = Input((13,4))  # zero everywhere and 1 for your cards\n",
    "    board = Input((3,13,4))  # 3 rounds of board: flop [0,:,:], turn [1,:,:] and river [2,:,:]\n",
    "    pot = Input((1,))\n",
    "    stack = Input((1,))\n",
    "    opponent_stack = Input((1,))\n",
    "    blinds = Input((2,))  # small, big blinds\n",
    "    dealer = Input((1,))  # 1 if you are the dealer, 0 otherwise\n",
    "    # opponent_model = Input((2,))  # tendency to raise, number of hands played: 2 numbers between 0 and 1\n",
    "    preflop_plays = Input((6, 5, 2))  # 6 plays max (check then 5 times raises), 5 possible actions (check,bet,call,raise,all-in), 2 players\n",
    "    flop_plays = Input((6, 5, 2))\n",
    "    turn_plays = Input((6, 5, 2))\n",
    "    river_plays = Input((6, 5, 2))\n",
    "    # value_of_hand = Input((2,))  # combination_type, rank_in_this_type\n",
    "\n",
    "    # Processing board and hand specifically to detect flush and straight\n",
    "    color_hand = Sum(1)(hand)\n",
    "    color_board = Sum((1,2))(board)\n",
    "    kinds_hand_for_ptqf = Sum(2)(hand)\n",
    "    kinds_hand_for_straight = IsNonZero()(kinds_hand_for_ptqf)\n",
    "    kinds_board_for_ptqf = Sum((1,3))(board)\n",
    "    kinds_board_for_straight = IsNonZero()(kinds_board_for_ptqf)\n",
    "\n",
    "    colors = Concatenate(2)([Reshape((4,1))(color_hand), Reshape((4,1))(color_board)])\n",
    "    kinds_for_straight = Concatenate(2)([Reshape((13,1))(kinds_hand_for_straight), Reshape((13,1))(kinds_board_for_straight)])\n",
    "    kinds_for_ptqf = Flatten()(Concatenate(2)([Reshape((13,1))(kinds_hand_for_ptqf), Reshape((13,1))(kinds_board_for_ptqf)]))\n",
    "    kinds_for_straight = Conv1D(5,1, activation=\"selu\", BN=False)(kinds_for_straight)\n",
    "    kinds_for_straight = Conv1D(1, 1, activation='selu', BN=False)(\n",
    "        Concatenate(-1)([\n",
    "            Conv1D(1, 3, activation='selu', BN=False)(kinds_for_straight),\n",
    "            Conv1D(3, 3, dilation_rate=2, activation='selu', BN=False)(kinds_for_straight)\n",
    "        ])\n",
    "    )\n",
    "    kinds_for_straight = Dense(hidden_dim, activation='selu', BN=False)(Flatten()(kinds_for_straight))\n",
    "\n",
    "    kinds_for_ptqf = Dense(hidden_dim, activation='selu', BN=False)(kinds_for_ptqf)\n",
    "    kinds_for_ptqf = Dense(hidden_dim, activation='selu', BN=False)(kinds_for_ptqf)\n",
    "    colors = Conv1D(1, 1, activation='selu', BN=False)(colors)\n",
    "    colors = Dense(hidden_dim, activation='selu', BN=False)(Flatten()(colors))\n",
    "\n",
    "    # Process board only\n",
    "    flop_alone = Dense(hidden_dim, activation='selu', BN=False)(Lambda(lambda x: x[:, 0, :, :])(board))\n",
    "    flop_alone = Dense(hidden_dim, activation='selu', BN=False)(flop_alone)\n",
    "    turn_alone = Dense(hidden_dim, activation='selu', BN=False)(Lambda(lambda x: x[:, 1, :, :])(board))\n",
    "    turn_alone = Dense(hidden_dim, activation='selu', BN=False)(turn_alone)\n",
    "    river_alone = Dense(hidden_dim, activation='selu', BN=False)(Lambda(lambda x: x[:, 2, :, :])(board))\n",
    "    river_alone = Dense(hidden_dim, activation='selu', BN=False)(river_alone)\n",
    "\n",
    "    board_alone = Dense(hidden_dim, activation='selu', BN=False)(Flatten()(Concatenate()([flop_alone, turn_alone, river_alone])))\n",
    "    board_alone = Dense(hidden_dim, activation='selu', BN=False)(board_alone)\n",
    "\n",
    "    # Process board and hand together\n",
    "    bh = Dense(hidden_dim, activation='selu', BN=False)(Concatenate()([Flatten()(board), Flatten()(hand)]))\n",
    "    bh = Dense(hidden_dim, activation='selu', BN=False)(bh)\n",
    "    bh = Dense(hidden_dim, activation='selu', BN=False)(Concatenate()([kinds_for_ptqf, kinds_for_straight, colors, board_alone, bh]))\n",
    "    bh = Dense(hidden_dim, activation='selu', BN=False)(bh)\n",
    "\n",
    "    n_combination = 9\n",
    "    probabilities_of_each_combination_board_only = Dense(n_combination, activation='softmax', BN=False)(bh)\n",
    "    probabilities_of_each_combination_board_hand = Dense(n_combination, activation='softmax', BN=False)(bh)\n",
    "    board_value = Dense(1, activation='sigmoid', BN=False)(bh)\n",
    "    board_hand_value = Dense(1, activation='sigmoid', BN=False)(bh)\n",
    "\n",
    "    # Add pot, blind, dealer, stacks\n",
    "    pbds = Dense(hidden_dim, activation='selu')(Concatenate()([pot, blinds, dealer, stack, opponent_stack]))\n",
    "    pbds = Dense(hidden_dim, activation='selu')(pbds)\n",
    "\n",
    "\n",
    "    # Process plays\n",
    "    processed_preflop = Dense(hidden_dim, activation='selu')(Flatten()(preflop_plays))\n",
    "    processed_flop = Dense(hidden_dim, activation='selu')(Concatenate()([Flatten()(flop_plays), Flatten()(flop_alone)]))\n",
    "    processed_turn = Dense(hidden_dim, activation='selu')(Concatenate()([Flatten()(turn_plays), Flatten()(flop_alone), Flatten()(turn_alone)]))\n",
    "    processed_river = Dense(hidden_dim, activation='selu')(Concatenate()([Flatten()(river_plays), Flatten()(flop_alone), Flatten()(turn_alone), Flatten()(river_alone)]))\n",
    "    # processed_opponent = Dense(hidden_dim, activation='prelu', BN=True)(opponent_model)\n",
    "    plays = Dense(hidden_dim, activation='selu')(Add()([processed_preflop,\n",
    "                                                        processed_flop,\n",
    "                                                        processed_turn,\n",
    "                                                        processed_river,\n",
    "    #                                                     processed_opponent\n",
    "                                                       ]))\n",
    "    plays = Dense(hidden_dim, activation='selu')(plays)\n",
    "\n",
    "    situation_with_opponent = Dense(hidden_dim, activation='selu')(Concatenate()([plays, pbds, bh]))\n",
    "    situation_with_opponent = Dense(hidden_dim, activation='selu')(situation_with_opponent)\n",
    "    Q_values = Dense(n_actions, activation=None)(situation_with_opponent)\n",
    "\n",
    "    Q = Model([hand, board, pot, stack, opponent_stack, blinds, dealer, preflop_plays, flop_plays, turn_plays, river_plays], [Q_values, probabilities_of_each_combination_board_only, probabilities_of_each_combination_board_hand, board_value, board_hand_value])\n",
    "    Q.compile(Adam(), ['mse', 'categorical_crossentropy', 'categorical_crossentropy', 'binary_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "    situation_with_opponent_pi = Dense(hidden_dim, activation='selu')(Concatenate()([plays, pbds, bh]))\n",
    "    situation_with_opponent_pi = Dense(hidden_dim, activation='selu')(situation_with_opponent_pi)\n",
    "    PI_values = Dense(n_actions, activation='softmax')(situation_with_opponent_pi)\n",
    "    PI = Model([hand, board, pot, stack, opponent_stack, blinds, dealer, preflop_plays, flop_plays, turn_plays, river_plays],\n",
    "               [PI_values, probabilities_of_each_combination_board_only, probabilities_of_each_combination_board_hand, board_value, board_hand_value])\n",
    "    PI.compile(Adam(), ['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'binary_crossentropy', 'binary_crossentropy'])\n",
    "    return Q, PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, PI = get_Q_and_PI_networks(hidden_dim=10, n_actions=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(Q, to_file='Q.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-468d41843bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pydot' is not defined"
     ]
    }
   ],
   "source": [
    "print(pydot.find_graphviz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
